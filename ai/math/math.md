# 绪论

> 写在开头, github 对 latex 支持不太好, 提供了 pdf 版供您查看

## 数学基础

### 数与数组

![img.png](img/img.png)

+ **标量 (Scalar)**：

   - 定义：标量是一个单一的数值，不带方向。
   - 举例：温度、体积、质量等都可以被表示为标量。
   - 属性：标量只有大小，没有方向。

+ **向量 (Vector)**：

   - 定义：向量是具有大小和方向的量。在数学中，它通常由一组有序的数值组成，并可以表示为一个箭头，其中箭头的长度代表大小，方向代表其方向。

   - 举例：速度、加速度、力都是向量，因为它们都有大小和方向。

   - 属性：向量有n个分量，在二维空间中它有两个分量（例如x和y），在三维空间中它有三个分量（x、y和z）。

   - 向量可以用矩阵表示

     > 一个向量 v 在二维空间中通常可以表示为：
     >
     > $$ \mathbf{v} = \begin{bmatrix} v_x \\ v_y \end{bmatrix} $$ 
     >
     > 在三维空间中为：
     >
     > $$ \mathbf{v} = \begin{bmatrix} v_x \\ v_y \\ v_z \end{bmatrix} $$
     >
     > 向量的方向通常由它的方向角来描述。在二维空间中，方向角是与正x轴之间的角。在三维空间中，还可以有与xy平面的夹角或与某个特定坐标轴的夹角。
     >
     > 对于二维空间，向量 v 的方向角 *θ* 可以通过其分量计算：
     >
     > $$ \theta = \tan^{-1}\left(\frac{v_y}{v_x}\right) $$
     >
     > 在三维空间中，情况稍微复杂一些，但基本的概念相似。你可以使用类似的方法来描述向量与坐标轴的夹角。
     >
     > 要注意的是，当我们说“方向”时，我们通常指的是一个无大小的概念，只关心向量“指向哪里”，而不关心它有多长。而当我们规范化一个向量时，我们得到一个所谓的`单位向量`，它在同一方向上，但其长度为1。

+ **张量 (Tensor)**：

   - 定义：张量是一个更为广泛的概念，可以被认为是标量、向量和更高维结构的泛化。

   - 属性：张量的一个关键属性是其"阶"（或"秩"）。标量是零阶张量，向量是一阶张量，矩阵是二阶张量。三阶张量可以被想象为一个数据立方体，但张量可以是任意阶的。

   - 本质可以描述为一个多维数字数组

   - 举例：

     > 图像数据通常被表示为一个4D张量：[批次大小, 高度, 宽度, 颜色通道]。
     >
     > 在物理学中，应力张量用于描述物体上的力的分布；
     >
     > 在深度学习中，张量是用于存储多维数据的主要数据结构，例如RGB图像可以被看作一个三阶张量。

+ **单位矩阵 (Identity Matrix)**：

   - 定义：单位矩阵是一个方阵，其中对角线上的元素都为1，而其它元素都为0。

   - 符号：通常用大写字母I表示。

   - 属性：对于任意矩阵A，当与单位矩阵相乘时，结果仍然是A，即 AI = IA = A。

   - 举例：

   $$ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ \end{bmatrix} $$

   

+ **分布式向量 (Distributed Vector)**

   - 又称为词嵌入 (word embeddings)。

   - 是将词汇映射到一个连续的向量空间中，每个维度都有具体的语义含义。

   - 常见的词嵌入模型包括 Word2Vec、GloVe 和 FastText。

   - 分布式向量捕获了词汇之间的语义关系。例如，相似的词汇在这个空间中可能会靠得很近。

   - 向量的维度通常远小于词汇表的大小，并且是固定的（如 50、100、200、300 维）。

   - 用一个具体的列子描述 分布式向量 : 

     > 假设我们有以下句子：
     >
     > 1. 猫喜欢吃鱼。
     > 2. 狗喜欢吃骨头。
     > 3. 猫经常捉老鼠。
     > 4. 狗喜欢追猫。
     >
     > 如果我们使用一个简化的词嵌入模型（如 Word2Vec）对这些句子进行训练，我们可能会得到每个词的分布式向量。这些向量会在一个多维空间中表示每个词的“含义”。
     >
     > 假设我们的向量是2维的（实际应用中的向量通常有更高的维度，例如100、200或300维），那么这些词的向量表示可能如下：
     >
     > - 猫: [0.7, 0.2]
     > - 狗: [0.6, 0.3]
     > - 喜欢: [0.1, 0.9]
     > - 吃: [0.2, 0.8]
     > - 鱼: [0.5, 0.1]
     > - 骨头: [0.4, 0.1]
     > - 捉: [0.9, 0.3]
     > - 老鼠: [0.8, 0.2]
     >
     > 从这个模拟中，我们可以观察到：
     >
     > 1. “猫”和“狗”的向量在空间中比较接近，表示它们在语义上有相似性，这是因为它们都与“喜欢”和“吃”有关。
     > 2. “鱼”和“骨头”的向量也接近，表示它们都是某种食物。
     > 3. “捉”和“老鼠”的向量也相对接近，因为它们经常在相同的语境中出现。
     >
     > 这只是一个简化的模型，实际的分布式向量会捕捉到更加复杂和细微的语义关系。在这样的空间中，通过词向量之间的相似性，我们可以推断出词语之间的语义关系，如找到最相似的词、进行词的类比等。

+ **独热向量 (One-hot Vector)**

   - 是表示分类变量的经典方法。对于每一个词汇，独热向量中该词汇的索引位置为1，其他位置为0。
   - 假设有一个大小为 N 的词汇表，那么每个词汇的独热向量的维度都是 N。
   - 例如，对于词汇表 ["apple", "banana", "cherry"]，"apple" 的独热表示为 [1, 0, 0]，"banana" 为 [0, 1, 0]，以此类推。
   - 独热向量不捕获词汇之间的任何语义关系。
   - 由于维度与词汇表的大小相同，对于大词汇表来说，独热编码可能非常稀疏和高维。

### 索引

![img.png](img/img2.png)

+ 三维张量A中的一个二维切片

   > 我们有一个三维张量 *A*，其尺寸为3×4×5, 表示如下
   > $$
   > \begin{bmatrix}
   > \begin{bmatrix}
   > a_{111} & a_{112} & a_{113} & a_{114} & a_{115} \\
   > a_{121} & a_{122} & a_{123} & a_{124} & a_{125} \\
   > a_{131} & a_{132} & a_{133} & a_{134} & a_{135} \\
   > a_{141} & a_{142} & a_{143} & a_{144} & a_{145} \\
   > \end{bmatrix} \\
   > 
   > \begin{bmatrix}
   > a_{211} & a_{212} & a_{213} & a_{214} & a_{215} \\
   > a_{221} & a_{222} & a_{223} & a_{224} & a_{225} \\
   > a_{231} & a_{232} & a_{233} & a_{234} & a_{235} \\
   > a_{241} & a_{242} & a_{243} & a_{244} & a_{245} \\
   > \end{bmatrix} \\
   > 
   > \begin{bmatrix}
   > a_{311} & a_{312} & a_{313} & a_{314} & a_{315} \\
   > a_{321} & a_{322} & a_{323} & a_{324} & a_{325} \\
   > a_{331} & a_{332} & a_{333} & a_{334} & a_{335} \\
   > a_{341} & a_{342} & a_{343} & a_{344} & a_{345} \\
   > \end{bmatrix}
   > \end{bmatrix}
   > $$
   >
   > 现在，如果我们想获取张量 *A* 的第二个二维切片，它将是：
   >
   > $$
   > \left[
   > \begin{array}{ccccc}
   > a_{211} & a_{212} & a_{213} & a_{214} & a_{215} \\
   > a_{221} & a_{222} & a_{223} & a_{224} & a_{225} \\
   > a_{231} & a_{232} & a_{233} & a_{234} & a_{235} \\
   > a_{241} & a_{242} & a_{243} & a_{244} & a_{245} \\
   > \end{array}
   > \right]
   > $$
   > 同样，你可以选择第一个或第三个切片，或沿其他轴的任何切片。
   > 

### 集合

![img3.png](img/img3.png)



> 复数是数学中的一种数，它由一个实部和一个虚部组成，并通常表示为 a+bi 的形式，其中 a 和 b 是实数，而 i 是虚数单位，满足 *$$i^2$$* =−1。

### 线性代数

![img3.png](img/img4.png)

+ 矩阵转置
   + 矩阵转置的操作是将矩阵的行转为列，列转为行。
   
   + $$ (A^T)^T = A $$ 
   
   + $$ (A + B)^T = A^T + B^T $$ 
   
   + $$ (AB)^T = B^T A^T $$, 涉及到链式法则和反向传播
   
     > $$ (AB)^T = B^T A^T $$ 用途
     >
     > - **计算效率**：有时，直接计算转置的乘积可能比先乘后转置更加高效。
     > - **线性代数证明**：这个性质常常用于证明其他矩阵恒等式。
     > - **在深度学习的反向传播中**：当进行链式法则的计算时，这个性质会出现。特别是当我们希望计算权重更新或计算梯度时，这个性质非常有用。
     
     > + 链式法则
     >
     >   链式法则是微积分中的一个基本定理，用于计算复合函数的导数。在多元微积分和深度学习中，链式法则尤为重要，因为它允许我们计算复杂函数的梯度。
     >
     >   在深度学习中，链式法则是反向传播算法的基础，用于计算神经网络中每一层的权重更新。通过链式法则，我们可以从输出层反向逐层计算每个参数的梯度。
     >
     >   ![Screenshot 2024-01-17 at 17.12.15](./img/Screenshot 2024-01-17 at 17.12.15.png)
     >
     > + 反向传播算法
     >
     >   反向传播算法是一种用于训练人工神经网络的方法，特别是在监督学习中。该算法通过计算损失函数相对于网络参数（通常是权重和偏差）的梯度来更新这些参数。反向传播结合了梯度下降优化算法，使得训练过程在减少预测误差方面更加有效。
     >
     >   反向传播算法的核心是通过网络的层次结构有效地传播误差，使得每个参数的更新都有助于减少整体的预测误差。
     >
     >   反向传播算法通常包括以下步骤：
     >
     >   1. **前向传播**：
     >      - 输入数据被送入网络，逐层传递直至输出层。
     >      - 在每一层，节点的输出是其输入的加权和，经过激活函数处理后的结果。
     >   2. **计算损失**：
     >      - 在输出层，网络的预测结果与实际标签进行比较，以计算损失（或误差）。损失函数的选择取决于任务的性质（例如，回归或分类）。
     >   3. **反向传播误差**：
     >      - 计算损失函数相对于每个参数的梯度。这一步骤涉及到链式法则，用于沿着网络反向传播误差，从输出层到输入层。
     >      - 为了计算这些梯度，反向传播算法首先计算输出层的误差，然后将误差反向传递回网络，以便计算隐藏层的误差。
     >   4. **更新参数**：
     >      - 一旦计算出损失相对于每个参数的梯度，就使用梯度下降（或其他优化算法）来更新网络的权重和偏差。
     >      - 参数更新的目的是最小化损失函数。
     >   5. **重复过程**：
     >      - 上述过程在每个训练批次中重复执行，直至网络的性能达到满意的水平。
     >
     >   ![Screenshot 2024-01-17 at 17.14.49](./img/Screenshot 2024-01-17 at 17.14.49.png)
     >
     > + 梯度下降优化算法
     >
     >   梯度下降优化算法是一种用于寻找函数最小值的迭代算法。在机器学习和深度学习中，梯度下降通常用于优化损失函数，即寻找能够最小化损失函数的模型参数。
     >
     >   梯度下降的基本思想是计算损失函数关于参数的梯度（或导数），然后按照梯度下降的方向调整参数。这个过程重复进行，直到找到损失函数的最小值或达到某个终止条件。
     >
     >   以下是梯度下降算法的基本步骤：
     >
     >   ![Screenshot 2024-01-17 at 17.26.30](/Users/vjf/Projects/github/project-LLM/LLM/img/Screenshot 2024-01-17 at 17.26.30.png)
     >
     >   ![Screenshot 2024-01-17 at 17.29.31](/Users/vjf/Projects/github/project-LLM/LLM/img/Screenshot 2024-01-17 at 17.29.31.png)
     >
     >   梯度下降的几个变体包括：
     >
     >   - **批梯度下降**（Batch Gradient Descent）：在整个训练集上计算梯度。
     >   - **随机梯度下降**（Stochastic Gradient Descent, SGD）：在每个样本上计算梯度。
     >   - **小批量梯度下降**（Mini-batch Gradient Descent）：在每个小批量样本上计算梯度。
     >
     >   学习率 *η* 的选择对算法的性能和收敛速度有很大影响。过大的学习率可能导致算法在最小值附近震荡甚至偏离，而过小的学习率会导致收敛速度很慢。
   
+ Hadamard乘积
   
   + 元素级别的乘积，即两个相同大小矩阵的相应元素进行相乘。
   + Hadamard乘积在神经网络中，尤其是在逐元素激活函数中有应用。例如，当计算ReLU激活函数的导数时，你可能会用到Hadamard乘积。
   + $$ C = A \odot B $$ $$其中, C_{ij} = A_{ij} \times B_{ij} $$
   
+ 矩阵的行列式
   + 对于一个方阵，其行列式是一个数值
   
   + $$ \text{det}(AB) = \text{det}(A) \times \text{det}(B) $$

   + $$ \text{det}(A^{-1}) = \frac{1}{\text{det}(A)} $$, 逆矩阵的行列式等于矩阵行列式的倒数
   
     ![Screenshot 2024-01-17 at 17.41.44](/Users/vjf/Projects/github/project-LLM/LLM/img/Screenshot 2024-01-17 at 17.41.44.png)
   
   + 行列式如何计算: 
   
     > + 对于2x2矩阵，计算行列式相对简单：
     >
     > $$ \text{det}\left( \begin{bmatrix} a & b \\ c & d \end{bmatrix} \right) = ad - bc $$
     >
     > 对于3x3矩阵，可以使用所谓的Sarrus法则。但对于更大的矩阵，我们通常使用Laplace展开或其他方法。
     >
     > + 使用Laplace展开计算3x3矩阵的行列式：
     >
     > $A = \begin{bmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{bmatrix}$
     >
     > 使用Laplace展开，我们可以通过第一行来展开该矩阵的行列式：
     >
     > $\det(A) = a_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13} $
     >
     > 其中，$C_{ij}$ 是矩阵 *A* 在第 *i* 行和第 *j* 列的余子式的代数余子式。具体地说，对于3x3矩阵，我们有：
     > $$
     > C_{11} = (-1)^{1+1} \det \left( \begin{bmatrix} a_{22} & a_{23} \\ a_{32} & a_{33} \end{bmatrix} \right) = a_{22}a_{33} - a_{23}a_{32} \\
     > 
     > C_{12} = (-1)^{1+2} \det \left( \begin{bmatrix} a_{21} & a_{23} \\ a_{31} & a_{33} \end{bmatrix} \right) = -1 *(a_{21}a_{33} - a_{23}a_{31})\\
     > 
     > C_{13} = (-1)^{1+3} \det \left( \begin{bmatrix} a_{21} & a_{22} \\ a_{31} & a_{32} \end{bmatrix} \right) = a_{21}a_{32} - a_{22}a_{31}
     > $$
     > 得到结果如下: 
     >
     > $$ \text{det}(A) = a \times \text{det}\left( \begin{bmatrix} e & f \\ h & i \end{bmatrix} \right) - b \times \text{det}\left( \begin{bmatrix} d & f \\ g & i \end{bmatrix} \right) + c \times \text{det}\left( \begin{bmatrix} d & e \\ g & h \end{bmatrix} \right) $$ 
     >
   
+ 矩阵与向量拼接

     + 如果我们有一个矩阵 *A* 和一个向量 *V*，我们可以将 *V* 作为 *A* 的新列拼接到 *A* 上

     + $$ B = [A | V] $$

     + 计算如下

       >矩阵与向量的拼接常指的是将一个向量作为一个新的列或行添加到矩阵中。这在很多数学和工程应用中都是有用的，例如在数据分析中将新数据点添加到数据集中。
       >
       >让我们通过一个示例来说明这一点。
       >
       >假设我们有矩阵 A 和向量 v：
       >
       >$$ A = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} $$ 
       >
       >$$ \mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} $$
       >
       >### 列拼接:
       >
       >将向量 **v** 作为新的列添加到矩阵 A 中，我们得到：
       >
       >$$ A' = \begin{bmatrix} a_{11} & a_{12} & v_1 \\ a_{21} & a_{22} & v_2 \end{bmatrix} $$
       >
       >### 行拼接:
       >
       >将向量 v 转置后作为新的行添加到矩阵 A 中，我们得到：
       >
       >$$ A'' = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \\ v_1 & v_2 \end{bmatrix} $$
       >
       >
       
       

+ 向量的点积
   + 点积为我们提供了两个向量之间的相似性信息。如果点积为正，那么两向量的方向相似；如果为零，则两向量正交；如果为负，则它们在不同方向上。

   + 代数定义: 向量的点积通常在代数上定义为两个向量相应分量的乘积之和, 对于二维或三维向量，这个定义是其对应元素的乘积之和。

   + 几何定义: 下面会详细阐述

     >### 代数定义:
     >
     >对于二维空间中的向量 **a** 和 **b**，它们通常表示为：
     >
     >$$ \mathbf{a} = \begin{bmatrix} a_1 \\ a_2 \end{bmatrix} $$ , $$ \mathbf{b} = \begin{bmatrix} b_1 \\ b_2 \end{bmatrix} $$
     >
     >$$ \mathbf{a} \cdot \mathbf{b} = a_1 b_1 + a_2 b_2 $$
     >
     >对于三维空间中的向量 **a** 和 **b**，它们可以表示为：
     >
     >$$ \mathbf{a} = \begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix} $$ $$ \mathbf{b} = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix} $$
     >
     >$$ \mathbf{a} \cdot \mathbf{b} = a_1 b_1 + a_2 b_2 + a_3 b_3 $$
     >
     >### 几何定义: 
     >
     >点积还与向量的长度有关。具体来说，两个向量的点积等于它们长度的乘积与它们之间的夹角的余弦值的乘积：
     >
     >$$ x \cdot y = ||x|| ||y|| \cos(\theta) $$
     >
     >其中, ||x|| ||y|| 是两个向量的长度（或模），而 *θ* 是两向量之间的夹角。
     >
     >+ 向量的长度（或模）
     >
     >给定一个向量 $$ \mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} $$ ，
     >
     >它的长度或模定义为：$$ ||\mathbf{v}|| = \sqrt{v_1^2 + v_2^2 + \dots + v_n^2} $$ 
     >
     >对于三维向量，这可以写作：$$ ||\mathbf{v}|| = \sqrt{v_1^2 + v_2^2 + v_3^2} $$
     >
     >+ 向量的夹角
     >
     >![Screenshot 2024-01-17 at 17.51.38](/Users/vjf/Projects/github/project-LLM/LLM/img/Screenshot 2024-01-17 at 17.51.38.png)
   
+ 其他问题

   + 矩阵和行列式的关系？

     行列式是与方阵（即行数和列数相同的矩阵）相关的一个数值标量。它为我们提供了关于该方阵某些重要性质的信息。例如，一个矩阵是可逆的（即存在逆矩阵）当且仅当它的行列式不为0。此外，行列式的几何意义是给出了由矩阵的列向量所形成的平行体的有向体积。

   + 向量为什么可以用矩阵表示? 

     一个向量可以被看作是一个特殊的矩阵。具体地说，一个列向量是一个 1×*n*×1 的矩阵，而一个行向量是一个 1×1×*n* 的矩阵。使用矩阵来表示向量的好处是我们可以应用矩阵运算（如矩阵乘法）来操作这些向量。

   + 什么叫矩阵的逆？

     > 如果一个方阵 *A* 的逆存在，那么它的逆矩阵是一个同样大小的方阵，记作$A^{-1}$，满足以下关系：
     >
     > $A A^{-1} = A^{-1} A = I$
     >
     > 其中 *I* 是单位矩阵，其对角线上的元素都是1，其他元素都是0。
     >
     > 只有当矩阵的行列式不为0时，矩阵才是可逆的。
   
    + 如何计算矩阵的逆？
   
      ![Screenshot 2024-01-17 at 17.55.26](/Users/vjf/Projects/github/project-LLM/LLM/img/Screenshot 2024-01-17 at 17.55.26.png)
      
      对于更大的矩阵，比如 3×33×3 或更高维度，可以使用以下方法：
      
      + **伴随矩阵法**
      
        - 计算原矩阵的伴随矩阵。
      
        - 将伴随矩阵的每个元素除以原矩阵的行列式。
      
          ![Screenshot 2024-01-18 at 14.11.32](/Users/vjf/Projects/github/project-LLM/LLM/img/Screenshot 2024-01-18 at 14.11.32.png)
      
          
      
      + 高斯-约当消元法
      
        ![Screenshot 2024-01-18 at 14.26.13](/Users/vjf/Library/Application Support/typora-user-images/Screenshot 2024-01-18 at 14.26.13.png)

### 微积分

![img.png](img/img5.png)

+ 导数常用公式

  > + 常数
  >
  >   $$ f(x) = c $$ $$ f'(x) = 0 $$ , c 为任意常数
  >
  > + **幂函数**
  >
  >   $$ f(x) = x^n $$ $$ f'(x) = nx^{n-1} $$ ,  *n* 是任意实数。
  >
  > + **指数函数**
  >
  >   $$ f(x) = e^x $$ $$ f'(x) = e^x $$
  >
  > + **自然对数函数**
  >
  >   $$ f(x) = \ln(x) $$ $$ f'(x) = \frac{1}{x} $$
  >
  > + **正弦函数**
  >
  >   $$ f(x) = \sin(x) $$ $$ f'(x) = \cos(x) $$
  >
  > + **余弦函数**
  >
  >   $$ f(x) = \cos(x) $$ $$ f'(x) = -\sin(x) $$
  >
  > + **正切函数**
  >
  >   $$ f(x) = \tan(x) $$ $$ f'(x) = \sec^2(x) $$
  >
  > + **泰勒展开公式 (Taylor Series)**
  >
  >   泰勒公式用于近似函数。对于在点 �*a* 可微的函数 �(�)*f*(*x*)，其泰勒展开式为：
  >
  > + **洛必达法则 (L'Hopital's Rule)**
  >
  >   当你有一个形如  $$\frac{0}{0}$$  或  $$\frac{\infty}{\infty}$$  的不定型极限时，洛必达法则可以派上用场。规则是这样的：
  >
  >   如果 $$\lim_{{x \to a}} f(x) = 0$$  和 $$\lim_{{x \to a}} g(x) = 0$$  或两者都趋向于无穷，且  $$g'(x) \neq 0$$  在某个区间中（除可能在 \(a\) 处之外），那么 $$\lim_{{x \to a}} \frac{f(x)}{g(x)} = \lim_{{x \to a}} \frac{f'(x)}{g'(x)}$$  如果这个新的极限存在或等于无穷。
  >

### 概率与信息论

![img.png](img/img6.png)

### 数据与概率分布

![img.png](img/img7.png)

### 函数

![img.png](img/img8.png)

### 本书中常用写法

![img.png](img/img9.png)











